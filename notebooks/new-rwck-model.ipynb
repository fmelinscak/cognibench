{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import sciunit\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ldmunit.models.RWCK import RWCKModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit import BanditTwoArmedHighLowFixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = dict(zip(['alpha', 'alpha_c', 'beta', 'beta_c', 'w0'], [1,1,1,1,0]))\n",
    "model = RWCKModel(2, 1, paras= paras)\n",
    "env = BanditTwoArmedHighLowFixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, model, n_trials, seed=0):\n",
    "    \"\"\"Simulation in a given AI Gym environment.\"\"\"\n",
    "    #TODO: add support for env.seed()\n",
    "    assert isinstance(env, gym.Env)\n",
    "    assert isinstance(model, sciunit.Model)\n",
    "    assert isinstance(n_trials, int)\n",
    "    \n",
    "    assert model.paras != None #TODO: add assert for keys\n",
    "\n",
    "\n",
    "    # reset the agent state and \n",
    "    model.reset()\n",
    "    init_stimulus = env._reset()\n",
    "\n",
    "    a = np.zeros(n_trials, dtype=int) #TODO: change dtype\n",
    "    r = np.zeros(n_trials, dtype=int)\n",
    "    s = np.zeros(n_trials, dtype=int)\n",
    "\n",
    "    # add the first stimulus in the environment\n",
    "    s = np.insert(s, 0, init_stimulus, axis=0)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # compute choice probabilities\n",
    "        P = model.predict(s[i])\n",
    "\n",
    "        # action based on choice probabilities\n",
    "        a[i] = model.act(P)\n",
    "\n",
    "        # generate reward based on action\n",
    "        s[i+1], r[i], done, _ = env._step(a[i])\n",
    "\n",
    "        # update choice kernel and Q weights\n",
    "        model.update(s[i], r[i], a[i], done)\n",
    "\n",
    "    # delete the extra stimulus\n",
    "    s = np.delete(s, n_trials, axis=0)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    obs = {\n",
    "        'stimuli': s,\n",
    "        'actions': a,\n",
    "        'rewards': r,\n",
    "    }\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(model, stimuli, rewards, actions):\n",
    "    #TODO: add assertion for the length\n",
    "    n_trials = len(stimuli)\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    # reset the model's state\n",
    "    model.reset()\n",
    "    done = False\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        # compute choice probabilities\n",
    "        P = model.predict(stimuli[i])\n",
    "        \n",
    "        # probability of the action\n",
    "        #TODO: generalize this\n",
    "        p = model.loglikelihood(P, actions[i])\n",
    "\n",
    "        # add log-likelihood\n",
    "        res += np.log(p)\n",
    "        # update choice kernel and Q weights\n",
    "        model.update(stimuli[i], rewards[i], actions[i], done)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, stimuli, rewards, actions, fixed):\n",
    "    \n",
    "    vals = list(fixed.values())\n",
    "\n",
    "    def objective_fun(vals):\n",
    "        for k, v in zip(fixed.keys(), vals):\n",
    "            model.paras[k] = v\n",
    "        return -loglike(model, stimuli, rewards, actions)\n",
    "\n",
    "    opt_results = scipy.optimize.minimize(fun=objective_fun,\n",
    "                                          x0=vals, \n",
    "                                          bounds=[(0,1000)]*len(vals))\n",
    "    if opt_results.success:\n",
    "        for k, v in zip(fixed.keys(), opt_results.x):\n",
    "            model.paras[k] = v\n",
    "    return opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = simulate(env, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likehood value -55.97948702891962\n"
     ]
    }
   ],
   "source": [
    "a = loglike(model, res['stimuli'], res['rewards'], res['actions'])\n",
    "print(\"Log-likehood value\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paras before minimize {'alpha': 1, 'alpha_c': 1, 'beta': 1, 'beta_c': 1, 'w0': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"paras before minimize\", model.paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/testing-ldmm/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "../src/ldmunit/models/RWCK.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x * beta) / np.sum(np.exp(x * beta), axis=0)\n"
     ]
    }
   ],
   "source": [
    "fixed = dict(zip(['alpha', 'alpha_c', 'beta_c', 'w0'], [1,1,1,0]))\n",
    "opt = fit(model, res['stimuli'], res['rewards'], res['actions'], fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paras after minimize {'alpha': 3.912657840187214, 'alpha_c': 0.0, 'beta': 1, 'beta_c': 1.4106348315533408, 'w0': 1e-08}\n"
     ]
    }
   ],
   "source": [
    "print(\"paras after minimize\", model.paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:testing-ldmm]",
   "language": "python",
   "name": "conda-env-testing-ldmm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
