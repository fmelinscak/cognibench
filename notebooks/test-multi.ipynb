{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import sciunit\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ldmunit.models import decision_making, associative_learning\n",
    "from src.ldmunit.models.utils import loglike, train_with_obs, simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit import BanditEnv, BanditAssociateEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "n_obs = 3\n",
    "n_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for random responding\n",
      "The log-likelihood:    -52.813\n",
      "[0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for random responding\")\n",
    "paras = {'bias': 0.8, 'action_bias': 1}\n",
    "\n",
    "model = decision_making.RandomRespondModel(n_actions=n_actions, n_obs=n_obs, paras=paras)\n",
    "env = BanditEnv([0.3, 0.7], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for noisy-win-stay-lose-shift\n",
      "The log-likelihood:    -71.356\n",
      "[0.8]\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for noisy-win-stay-lose-shift\")\n",
    "paras = {'epsilon': 0.8}\n",
    "\n",
    "model = decision_making.NWSLSModel(n_actions, n_obs, paras)\n",
    "env = BanditEnv([0.3, 0.7], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for RWCK\n",
      "The log-likelihood:    -70.383\n",
      "[0.1 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for RWCK\")\n",
    "paras = {'w0': 0.1, 'alpha': 0.5, 'alpha_c': 0.5, 'beta': 0.5, 'beta_c': 0.5}\n",
    "\n",
    "model = decision_making.RWCKModel(n_actions, n_obs, paras)\n",
    "env = BanditEnv([0.3, 0.7], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ldmunit.models.utils import MultiMeta, multi_from_single\n",
    "\n",
    "param_list = [{'epsilon': 0.5}] * 10\n",
    "MultiNWSLS = multi_from_single(decision_making.NWSLSModel, 'MultiNWSLS')\n",
    "models = MultiNWSLS(param_list, n_actions=2, n_obs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sciunit.models.base.Model,\n",
       " src.ldmunit.capabilities.Interactive,\n",
       " src.ldmunit.capabilities.DiscreteAction,\n",
       " src.ldmunit.capabilities.DiscreteObservation)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiNWSLS.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.act(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associate learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 3\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for RwNormModel\n",
      "The log-likelihood:    -24.132\n",
      "      fun: 48.82391260704972\n",
      " hess_inv: array([[1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 1]])\n",
      "      jac: array([3.84012210e+08, 5.50318936e+08, 6.22704006e+08, 6.54975433e+08,\n",
      "       6.70113864e+08])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 208\n",
      "      nit: 0\n",
      "     njev: 28\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0.1, 0.5, 0.5, 0.5, 0.5])\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for RwNormModel\")\n",
    "paras = {'w0': 0.1, 'alpha': 0.5, 'sigma': 0.5, 'b0': 0.5, 'b1': 0.5}\n",
    "\n",
    "model = associative_learning.RwNormModel(n_obs, paras)\n",
    "env = BanditAssociateEnv([0.3, 0.7, 0.8], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for KrwNormModel\n",
      "The log-likelihood:    -6.1348\n",
      "      fun: 11.378747009038657\n",
      " hess_inv: array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "      jac: array([1.04240638e+08, 1.48914390e+08, 1.66735928e+08, 1.73683937e+08,\n",
      "       1.76371357e+08, 1.77408107e+08, 1.77807763e+08, 1.77961807e+08])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 332\n",
      "      nit: 0\n",
      "     njev: 32\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0.1 , 0.5 , 0.5 , 0.5 , 0.5 , 0.23, 0.32, 0.34])\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for KrwNormModel\")\n",
    "paras = {'w0': 0.1, 'alpha': 0.5, 'sigma': 0.5, 'b0': 0.5, 'b1': 0.5, 'logSigmaWInit': 0.23, 'logTauSq': 0.32,\n",
    "        'logSigmaRSq': 0.34}\n",
    "\n",
    "model = associative_learning.KrwNormModel(n_obs, paras)\n",
    "env = BanditAssociateEnv([0.3, 0.7, 0.8], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for lsspd\n",
      "The log-likelihood:    -4.4083\n",
      "      fun: 4.120390217262259\n",
      " hess_inv: array([[ 0.99918541, -0.00332696, -0.00607426, -0.00855724, -0.01067363,\n",
      "        -0.0124462 , -0.01392631, -0.01516451],\n",
      "       [-0.00332696,  0.98641204, -0.02480846, -0.03494944, -0.04359319,\n",
      "        -0.05083272, -0.05687778, -0.06193483],\n",
      "       [-0.00607426, -0.02480846,  0.95470552, -0.06380956, -0.07959104,\n",
      "        -0.09280874, -0.1038456 , -0.11307861],\n",
      "       [-0.00855724, -0.03494944, -0.06380956,  0.91010692, -0.11212557,\n",
      "        -0.13074628, -0.14629469, -0.15930189],\n",
      "       [-0.01067363, -0.04359319, -0.07959104, -0.11212557,  0.86014337,\n",
      "        -0.16308264, -0.18247651, -0.19870067],\n",
      "       [-0.0124462 , -0.05083272, -0.09280874, -0.13074628, -0.16308264,\n",
      "         0.8098342 , -0.21278042, -0.23169892],\n",
      "       [-0.01392631, -0.05687778, -0.1038456 , -0.14629469, -0.18247651,\n",
      "        -0.21278042,  0.76191562, -0.25925268],\n",
      "       [-0.01516451, -0.06193483, -0.11307861, -0.15930189, -0.19870067,\n",
      "        -0.23169892, -0.25925268,  0.71769692]])\n",
      "      jac: array([ 4636.48388356,  9064.46875119, 13293.1644668 , 17331.38659132,\n",
      "       21187.5722611 , 24869.79551399, 28385.78207195, 31742.9235934 ])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 232\n",
      "      nit: 1\n",
      "     njev: 22\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0.10000016, 0.50000065, 0.50000118, 0.50000167, 0.50000208,\n",
      "       0.23000242, 0.32000271, 0.34000295])\n"
     ]
    }
   ],
   "source": [
    "print(\"testing for lsspd\")\n",
    "paras = {'w0': 0.1, 'alpha': 0.5, 'sigma': 0.5, 'b0': 0.5, 'b1': 0.5, 'eta': 0.23, 'kappa': 0.32,\n",
    "        'mix_coef': 0.34}\n",
    "\n",
    "model = associative_learning.LSSPDModel(n_obs, paras)\n",
    "env = BanditAssociateEnv([0.3, 0.7, 0.8], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing for beta binomial\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-851488480b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massociative_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBetaBinomialModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBanditAssociateEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloglike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The log-likelihood: {:10.5}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/github/testing-ldmm/src/ldmunit/models/utils.py\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(env, model, n_trials, seed)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# generate reward based on action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0ms_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstimuli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/github/testing-ldmm/notebooks/bandit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"testing for beta binomial\")\n",
    "paras = {'b0': 0.5, 'b1': 0.5, 'mix_coef': 0.34}\n",
    "\n",
    "model = associative_learning.BetaBinomialModel(n_obs, paras)\n",
    "env = BanditAssociateEnv([0.3, 0.7, 0.8], [1,1])\n",
    "stimuli, rewards, actions = simulate(env, model, n_trials)\n",
    "a = loglike(model, stimuli, rewards, actions)\n",
    "print(\"The log-likelihood: {:10.5}\".format(a))\n",
    "\n",
    "opt = train_with_obs(model, stimuli, rewards, actions)\n",
    "if opt.success:\n",
    "    print(opt.x)\n",
    "else:\n",
    "    print(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:testing-ldmm]",
   "language": "python",
   "name": "conda-env-testing-ldmm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
