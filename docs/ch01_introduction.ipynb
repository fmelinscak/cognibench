{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to CogniBench\n",
    "CogniBench is a framework for benchmarking cognitive models using behavioral data. It is built mainly on top of [sciunit](https://github.com/scidash/sciunit) and [gym](https://github.com/openai/gym) libraries. It uses the same test-model-capability categorization implemented in sciunit to run test suites consisting of several tests on a set of models. For a full list of features, please refer to README.md file or cognibench documentation website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example: Testing multiple models interactively\n",
    "As a toy example, we test three models interactively. Let us first import the models. cognibench offers some single-subject model implementations. Tests can be used with single- or multi-subject models. In this section, we showcase testing multi-subject models and the automatic multi-subject creation from single-subject implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to suppress sciunit config not found logs\n",
    "import sciunit\n",
    "import os\n",
    "sciunit.settings['CWD'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognibench.models.decision_making import RandomRespondModel, NWSLSModel\n",
    "from cognibench.models.utils import multi_from_single_cls\n",
    "\n",
    "MultiRandomRespondModel = multi_from_single_cls(RandomRespondModel)\n",
    "MultiNWSLSModel = multi_from_single_cls(NWSLSModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need data to run the tests. In general, the type of data highly depends on the particular model. In this example, we assume that data is stored in `observations` variable. In later chapters, we will explain this part in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [{'stimuli': [], 'actions': [], 'rewards': []}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cognibench offers the `cognibench.testing.InteractiveTest` class for interactive tests. This class tests a given model using `(stimulus, action, reward)` tuples by first showing the stimulus to the model, then getting the prediction and finally returning the true `(stimulus, action, reward)` tuple. Therefore, the model has the chance to update itself after making a prediction on a stimulus.\n",
    "\n",
    "`InteractiveTest` class requires the observations, the score type to use and in this case the boolean switch to signal that we are using multi-subject models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognibench.testing import InteractiveTest\n",
    "from cognibench.scores import NLLScore\n",
    "from cognibench.utils import partialclass\n",
    "\n",
    "test = InteractiveTest(\n",
    "    name='Interactive negative log-likelihood test',\n",
    "    observation=observations,\n",
    "    score_type=partialclass(NLLScore, min_score=0, max_score=1e4),\n",
    "    multi_subject=True,\n",
    "    optimize_models=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the models. All of the models we have imported in this tutorial operate on discrete action and discrete observation spaces. Therefore, we need to specify the dimension for these spaces. In addition, each model require certain parameters. Here we assume that parameters are already set beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim_action = 5\n",
    "ndim_observation = 8\n",
    "num_subjects = 5\n",
    "\n",
    "multi_rr = MultiRandomRespondModel(n_subj=num_subjects, n_action=ndim_action, n_obs=ndim_observation)\n",
    "multi_nwsls = MultiNWSLSModel(n_subj=num_subjects, n_action=ndim_action, n_obs=ndim_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the test on the list of models. Since we don't have any observations in this example, both scores are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Executing test <i>Interactive negative log-likelihood test</i> on model <i>RandomRespondModel</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">0</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing test <i>Interactive negative log-likelihood test</i> on model <i>NWSLSModel</i>... "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Score is <a style=\"color: rgb(60,169,88)\">0</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interactive negative log-likelihood test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomRespondModel</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWSLSModel</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Interactive negative log-likelihood test\n",
       "RandomRespondModel                                        0\n",
       "NWSLSModel                                                0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_suite = sciunit.TestSuite([test], name='Test suite')\n",
    "model_list = [multi_rr, multi_nwsls]\n",
    "test_suite.judge(model_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ldmunit]",
   "language": "python",
   "name": "conda-env-ldmunit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
